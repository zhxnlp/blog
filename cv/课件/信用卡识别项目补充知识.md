# 信用卡识别项目补充知识

### 1. 模板匹配

模板匹配和卷积原理很像，模板在原图像上从原点开始滑动，计算模板与（图像被模板覆盖的地方）的差别程度，这个差别程度的计算方法在opencv里有6种，然后将每次计算的结果放入一个矩阵里，作为结果输出。假如原图形是AxB大小，而模板是axb大小，则输出结果的矩阵是(A-a+1)x(B-b+1)

- TM_SQDIFF：计算平方不同，计算出来的值越小，越相关        
- TM_CCORR：计算相关性，计算出来的值越大，越相关
- TM_CCOEFF：计算相关系数，计算出来的值越大，越相关
- TM_SQDIFF_NORMED：计算归一化平方不同，计算出来的值越接近0，越相关
- TM_CCORR_NORMED：计算归一化相关性，计算出来的值越接近1，越相关
- TM_CCOEFF_NORMED：计算归一化相关系数，计算出来的值越接近1，越相关

[公式](https://docs.opencv.org/3.3.1/df/dfb/group__imgproc__object.html#ga3a7850640f1fe1f58fe91a2d7583695d)

建议使用归一化的计算方法会相对公平一些.

- matchTemplate(image, templ, method[, result[, mask]]) 进行模板匹配
  - image是要匹配的图片
  - templ是模板图片
  - method是计算方式
  - result是进行匹配计算后得到的矩阵. 
  - mask是掩膜
- minMaxLoc(src[, mask])  获取最大值和最小值的位置
  - 返回四个值, 分别是最小值, 最大值, 最小值坐标, 最大值坐标

``` python
import cv2
import numpy as np
img = cv2.imread('lena.jpg', 0)
template = cv2.imread('face.jpg', 0)

res = cv2.matchTemplate(img, template, cv2.TM_SQDIFF)
print(res.shape)

min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)
print(min_loc)
```

展示6种匹配计算方式的不同:

``` python
import cv2
import numpy as np
img = cv2.imread('lena.jpg', 0)
template = cv2.imread('face.jpg', 0)

methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR',
           'cv2.TM_CCORR_NORMED', 'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED']

for meth in methods:
    img2 = img.copy()

    # 匹配方法的真值
    method = eval(meth)
    print (method)
    res = cv2.matchTemplate(img, template, method)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)

    # 如果是平方差匹配TM_SQDIFF或归一化平方差匹配TM_SQDIFF_NORMED，取最小值
    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
        top_left = min_loc
    else:
        top_left = max_loc
    bottom_right = (top_left[0] + w, top_left[1] + h)

    # 画矩形
    cv2.rectangle(img2, top_left, bottom_right, 255, 2)

    plt.subplot(121), plt.imshow(res, cmap='gray')
    plt.xticks([]), plt.yticks([])  # 隐藏坐标轴
    plt.subplot(122), plt.imshow(img2, cmap='gray')
    plt.xticks([]), plt.yticks([])
    plt.suptitle(meth)
    plt.show()
```

![](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/533/1630072391000/0ff19a11c5954517873a0d7dc801447b.jpg)

**匹配多个对象**

``` python
img_rgb = cv2.imread('mario.jpg')
img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)
template = cv2.imread('mario_coin.jpg', 0)
h, w = template.shape[:2]

res = cv2.matchTemplate(img_gray, template, cv2.TM_CCOEFF_NORMED)
threshold = 0.8
# 取匹配程度大于%80的坐标
loc = np.where(res >= threshold)
for pt in zip(*loc[::-1]):  # *号表示可选参数
    bottom_right = (pt[0] + w, pt[1] + h)
    cv2.rectangle(img_rgb, pt, bottom_right, (0, 0, 255), 2)

cv2.imshow('img_rgb', img_rgb)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

![](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/533/1630072391000/54f81b176da54173aab9571926f0d85e.png)



### 2. Otsu阈值

当阈值范围无法人工确定时, 可以使用Otsu的方法通过计算确定阈值.

Otsu适用于图片的灰度直方图是双峰结构的图形.

``` python
import cv2 
import numpy as np
import matplotlib.pyplot as plt

naza = cv2.imread('naza.png')
naza_gray = cv2.cvtColor(naza, cv2.COLOR_BGR2GRAY)
_ = plt.hist(naza_gray.ravel(), bins=256, range=[0, 255])

# 普通阈值处理
ret, dst = cv2.threshold(naza_gray, 80, 255, cv2.THRESH_BINARY)

cv2.imshow('naza', np.hstack((naza_gray, dst)))

# ostu阈值处理
ret, dst = cv2.threshold(cat_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

cv2.imshow('otsu', np.hstack((cat_gray, dst)))
cv2.waitKey(0)
cv2.destroyAllWindows()
```

![](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/533/1630072391000/ac5267e20fc74a44bdf34f1a2a54838a.png)

![](https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/533/1630072391000/acadb45c3d3944fc86c311859e3bc9fc.png)

### 3. 信用卡数字识别

**基本思路:**

总体思路就是取出信用卡中每一个数字作为去和模板中的10个数字进行模板匹配操作.

1. 先对模板处理, 获取每个数字的模板及其对应的数字标识.

2. 再对信用卡处理, 通过一系列预处理操作, 取出信用卡数字区域. 

3. 然后再取出每一个数字去和模板中的10个数字进行匹配.

   ​

