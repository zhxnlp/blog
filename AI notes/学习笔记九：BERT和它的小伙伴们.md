车万翔《基于预训练模型的自然语言处理》读书笔记
@[toc]
## 1.BERT的可解释性（7.5）
&#8195;&#8195;对很多实际应用而言，模型的性能和可解释性都很重要。BERT类的预训练模型体量庞大，参数众多，预测行为难以理解和不可控。

&#8195;&#8195;解释性：以人类的视角理解模型的行为。NLP中最具解释性的概念系统是**语言学特征**。

可解释性分两种：
- 自解释性模型（self-explainable）：模型构建之初就针对性设计其结构，使其具备可解释性
- 模型行为的事后解释，BERT等大规模预训练模型属于此种

&#8195;&#8195;本节从自注意力和表示学习两个角度分析BERT模型
### 1.2 自注意力可视化分析（不同注意力头的行为）
&#8195;&#8195;==自注意力的本质事实上是对词(或标记)与词之间关系的刻画==。不同类型的关系可以表达丰富的语义,例如名词短语内的依存关系、句法依存关系和指代关系等。自注意力的分析将有助于理解 BERT 模型对于关系( relational） 特征的学习能力。

**自注意力反映了预训练模型内部信息的聚合过程,而模型的各层隐含层表示是聚合的结果**

1. 可视化不同自注意力头的行为。可以看出,不同的注意力头具有比较多样化的行为,因而能够编码不同类型的上下文和关系特征。
*（有注意力头分布较为均匀,具有较大的感受野,即编码了较“分散”的上下文信息,有些注意力头的注意力分布较为集中,且显示出一定的模式,如集中在当前词下一个词,或者[ SEP 、句号等标记上）*

2. 计算各层注意力分布的信息熵，进一步分析了不同层的注意力分布。信息熵变化趋势反映 BERT 模型中信息聚合(或语义组合)的过程。
- 浅层（1-4）：注意力头具有较大的熵值(接近平均分布)，接近于词袋分布
- 中层（6-8）：其分布相为集中,熵值较小。形成集中在不同局部的注意力分布
- 深层（10-12）：熵值又增大。自注意力分布与目标预训练任务直接相关

### 1.3 探针实验
- 探针实验：定量分析，理解模型的行为。
- 探针：通常是一个非参或者非常轻量的参数模型（如线性分类器）
- 核心思想：设计特定的探针,对于待分析对象（如自注意力或隐含层表示）进行特定行为分析

1. 检验某个自注意力头对直接宾语( Direct object , dobj )关系的表达能力,可以设计个探针对该自注意力头在 dobj 句法关系预测上的表现进行分析。结果表明在 BERT 模型中,确实存在一部分自注意力头较好地捕捉到特定的句法关系。例如,对于 dobj 关系的预测准确率达到了86.8%。此外,对于更复杂的共指关系( Coreference ),同样能够找到具有较好预测能力的自注意力头。

2. 对预训练编码器的隐含层表示直接进行探针实验,理解其特性。探针为简单的线性分类器。该分类器利用模型的隐含层表示作为特征在目标任务(如词性标注)上训练,从而根据该任务的表现对预训练模型隐含层表示中蕴含的语言学特征评估。

可视化还可以参考： [《用可视化解构BERT，我们从上亿参数中提取出了6种直观模式》](https://mp.weixin.qq.com/s/IN4YfoZnlBozwEFdhSvLZg)
# 二、模型优化
## 1.1 XLNet
XLNet是一种基于 Transformer -XL 的自回归语言模型,并集成了自编码语言模型的优点：
- 用了自回归语言模型结构,使得各个单词的预测存在依赖性,同时避免了自编码语言模型中引入人造标记[ MASK ]的问题
- 引入了自编码语言模型中的双向上下文,能够利用更加丰富的上下文信息,
- 使用了 Transformer —XL作为主体框架,相比传统的 Transformer 拥有更好的性能。

&#8195;&#8195;为了在自回归模型中引入自编码模型的优点，XLNet引入两个最重要的改进方法：排列语言模型和双流自注意力机制。
### 1.1.2 排列语言模型的引入
&#8195;&#8195;传统的自回归语言模型,其中预测每一个单词需要依赖其历史词，而不能利用未来词。所以是单向语言建模。比如一个句子含有1234四个词，句子建模顺序是：
$$1-2-3-4$$
==为了构建双向上下文，引入排列语言模型==。例如建模顺序改为：
$$3-2-4-1$$
此时，整个句子建模为：
$$P(x)=P(x_{3})P(x_{2}|x_{3})P(x_{4}|x_{3}x_{2})P(x_{1}|x_{3}x_{2}x_{4})$$
&#8195;&#8195;可以看到,当预测$x_{4}$时,需要依赖$x_{3}$和$x_{2}$。而当预测$x_{1}$时,需要依赖$x_{4}$、$x_{3}$和$x_{2}$,即实现了双向上下文的建模方式。

&#8195;&#8195;排序语言模型：对于给定长度为N的句子，从所有可能的排列方式$\mathbb{Z}_{n}$中均匀地采样出一种排序z来构建语言模型。
&#8195;&#8195;我们用$z_{i}$表示排列z下，下一个预测词$x_{zi}$在句子中的下标。则建模需要最大化似然对数函数：$$\sum_{i=1}^{N}P(x_{zi}|x_{1:i-1},z_{i})$$
&#8195;&#8195;概率分布必须依赖位置$z_{i}$。而传统的建模根据词向量和前i-1个词的隐含层表示，直接求softmax函数下的最大概率就是预测的结果。这样没有用到位置$z_{i}$。即对于不同的排序z，会产生一样的概率分布，无法满足建模要求。
&#8195;&#8195;此时引入函数g，使概率分布依赖于目标位置$z_{i}$。
1.1.3 双流自注意力机制
 &#8195;&#8195;双流自注意力机制：同一个单词有以下两种表示：
 - 内容表示$h_{zi}$：传统的transformer表示方法，用于预测下一个词时提供完整上下文信息
 - 查询表示$g_{zi}$：建模上下文信息$x_{1:i-1}$及目标位置$z_{i}$，但看不到单词$x_{i}$。（只能利用位置信息而非单词，否则g知道了具体的单词可以直接输出了）
&#8195;&#8195;双流自注意力通过改变自注意力矩阵实现（Attention Mask）。

## 1.2 RoBERTa
&#8195;&#8195;RoBERTa作者设计详尽试验，进一步理解BERT设计的合理性，通过试验方法优化BERT。
&#8195;&#8195;BERT的原始输入模式是“文本对输入+NSP”。每个文本由多个句子组成，整个句子长度不超过512个token。如果使用“句子对+NSP”，会带来一定的性能损失，因为句子对长度较短，无法学习到长距离依赖，对阅读理解等任务带来较大影响。
1. ==动态掩码技术==：掩码位置和方法是在训练阶段实时计算，保证同一短文本在不同轮数下产生不同的掩码模式，提高数据的复用率（轮数或者数据量大的时候）。使用动态掩码技术的BERT在阅读理解、文本分类任务上，性能微弱提升
2. 舍弃NSP任务，采用==跨文档整句输入==的模式：由一对文本构成输入序列，到达文档末端时，继续从下一个文档抽取句子，并添加分隔符[SEP]表示文档边界。不使用“文档内整句输入”会导致batch_size是一个可变量，对于大规模训练并不友好。
3. 更大的预训练

项目    |BERT  | RoBERTa
-------- | ----- | ----
预训练数据 | BookCroups(16G) | 5个数据来源160G）
训练程度  | batch_size=256，steps=1M | batch_size=8k，steps=5M
词表  | WordPiece(30k) | SentencePiece(50k),不会出现未登录词

## 1.3 ALBERT（降低BERT参数+SOP）
- 词向量的作用是将输人文本映射到上下文无关的静态事示中,而BERT之所以强大，是因为Transformer 模型能够充分地学习到每个标记的上下文信息。故Transformer 的隐含层维度 H 要远大于词向量维度 E ,即 $H 》E$。
- 词向量矩阵的参数量是词表大小 V 乘以词向量维度 E，即O（VE）,通常情况下,词表大小 V 是比较大的(BERT是V=30K），早期中通过增大H提升模型性能时，E也会增大。
- 词向量矩阵的更新是比较稀疏的,参数的利用率并不高。
- BERT中多层transformer参数不共享，每层都有自己的参数

为了降低BERT模型参数量和优化NSP任务，ALBERT优化了三点：
1. 词向量因式分解：解耦E和H，引入一个全连接层，将词向量维度E映射到Transformer 的隐含层维度 H。词向量部分复杂度从O（VH）降到O（VE+EH）。$H 》E$时餐数量明显降低
2. 跨层参数共享：每个transformer block都有一样的权重，通过循环这个层复用参数，实现深层计算（循环多少次就是多少层）（降低参数量和磁盘占用，但不会减少内存和计算时间。因为模型训练推断时，共享参数还是要虚拟复制成多份，模型还是要从底层前向传播到顶层。）
3. 句子顺序预测：正例还是两个顺序的句子，负例是两个句子对调。这样比NSP任务难度更大，可以学到细微的语义差别及语篇连贯性。
## 1.4 ELECTRA
暂略
## 1.5 MacBERT
BERT中训练时mask而下游任务没有mask，造成预训练-精调不一致。哈工大提出基于文本纠错的掩码语言模型MacBERT，改变掩码方式：
1. 整词掩码+N-gram掩码：unigram到4-gram的概率分别是：40%，30%，20%，10%。
2. 使用相似词替换mask标记：使用同义词词典获得待掩码单词的同义词，N-gram掩码时对每个词进行同义词替换，没有同义词时随机词替换。
3. 保持BERT的掩码比例15%，其中80%替换为同义词，10%随机词，10%不替换。
4. SOP任务代替NSP任务
# 2. 长文本处理

















